{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68670c24",
   "metadata": {},
   "source": [
    "NoteBook to try some example algorithms in python from the reinforcement learning book. Originally the algorithms I implemented on MATLAB have been translated into python code. \n",
    "The basis problem is a grid of squares where the terminal states are square 1,and the final square. The policy implemented contains a grid of numbers, where in each square is the index the player should move to from that square.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4172c",
   "metadata": {},
   "source": [
    "# The Actual Value Function (for the policy pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c5ddb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "V = np.zeros((9,1))                #arbitrary initial V estimate  \n",
    "R = np.ones((9,1))                    \n",
    "R[0], R[8] = 0, 0                  #reward array\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "pi = np.array([0,0,1,0,1,8,7,8,8]) #policy array - each number points to the next location\n",
    "\n",
    "P = np.array([[1,0,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,1,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,0,1]]) #Prob matrix of policy\n",
    "\n",
    "VActual = np.matmul(np.linalg.inv(np.eye(9)-gamma*P),R) #Solve bellman equation\n",
    "VActual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680251d",
   "metadata": {},
   "source": [
    "# TD[0] (One Step TD) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34defc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the estimate and the actual value function is : \n",
      " [[ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [-1.42195706e-08]\n",
      " [-1.22070312e-04]\n",
      " [-9.69236730e-07]\n",
      " [-1.52587891e-05]\n",
      " [-4.75538693e-07]\n",
      " [-1.77635684e-15]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "V = np.zeros((9,1))                #arbitrary initial V estimate  \n",
    "R = np.ones((9,1))                    \n",
    "R[0], R[8] = 0, 0                  #reward array\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "#initially use for loops though I'm aware it's not the fastest way.\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    running = True         #true until the terminal state is reached\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Snew = pi[S]\n",
    "            V[S] = V[S] + alpha*(R[S]+gamma*V[Snew]-V[S])\n",
    "            S = Snew\n",
    "\n",
    "print(\"The difference between the estimate and the actual value function is : \\n\", V-VActual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a0b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db66e6c2",
   "metadata": {},
   "source": [
    "# Simple 3x3 Grid Q Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23545",
   "metadata": {},
   "source": [
    "Slightly different implementation of the 3x3 grid idea, where the Q table has actions 0-3; up, right, down, left in that order ascending. Rows of the Q table are the states and the columns are the possible actions. This method learns the optimum policy as the iterations of episodes update the Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ec78f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q: [[ 0.          0.          0.          0.        ]\n",
      " [-5.         -1.0875     -1.65399219 -1.        ]\n",
      " [-5.         -5.         -1.899746   -1.89973215]\n",
      " [-1.         -1.2        -1.239375   -5.        ]\n",
      " [-1.88683733 -1.88131438 -1.88529925 -1.88679201]\n",
      " [-1.18875    -5.         -1.         -1.63121094]\n",
      " [-1.89987183 -1.8998661  -5.         -5.        ]\n",
      " [-1.42625    -1.         -5.         -1.0875    ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "optimum policy:\n",
      " [[0. 0. 1.]\n",
      " [0. 5. 8.]\n",
      " [7. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "def get_max_A(arr):\n",
    "    indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "    return indices[0][0]\n",
    "\n",
    "#takes input of action and returns new state according to board physics\n",
    "def change_state(S,A):\n",
    "    #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "    if((S % 3 == 0 and A == 3) or (S < 3 and A == 0) or (S > 5 and A == 2) or (S % 3 == 2 and A == 1)):\n",
    "        return S\n",
    "    elif(A == 0):\n",
    "        return S-3\n",
    "    elif(A == 1):\n",
    "        return S+1\n",
    "    elif(A == 2):\n",
    "        return S+3\n",
    "    elif(A == 3):\n",
    "        return S-1\n",
    "    else:\n",
    "        print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "        return S\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "\n",
    "Q = np.zeros((9,4))                #arbitrary initial Q Table estimate ((9 states, 4 actions) value function)\n",
    "\n",
    "#set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward=-1\n",
    "R = np.array([[-10, -1, -1, -10],\n",
    "              [-10, -1, -1,  -1],\n",
    "              [-10,-10, -1,  -1],\n",
    "              [ -1, -1, -1, -10],\n",
    "              [ -1, -1, -1,  -1],\n",
    "              [ -1,-10, -1,  -1],\n",
    "              [ -1, -1,-10, -10],\n",
    "              [ -1, -1,-10,  -1],\n",
    "              [ -1,-10,-10,  -1]])\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    Snew = S               #Set Snew to fix the algorithm\n",
    "    running = True         #true until the terminal state is reached (S = 0 or 8)\n",
    "    # iteration = 1\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Amax = get_max_A(Q[S,:])       #Get action that maximizes current move\n",
    "            Snew = change_state(S,Amax)    #Find out the new state according to the action\n",
    "            Anewmax = get_max_A(Q[Snew,:]) #Get action that maximizes the next move\n",
    "            Q[S,Amax] = Q[S,Amax] + alpha*(R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "            S = Snew                         #Update state\n",
    "            # print('Iteration {} at state {}'.format(iteration,S)) #debugging iteration check\n",
    "            # iteration += 1\n",
    "\n",
    "print('Final Q: {}'.format(Q))\n",
    "#Find the optimum policy from Q:\n",
    "optimum_pi = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimum_pi[j,i] = change_state(i+j*3,get_max_A(Q[i+j*3,:]))\n",
    "optimum_pi[2,2] = 8\n",
    "print(\"optimum policy:\\n {}\".format(optimum_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fe6ce",
   "metadata": {},
   "source": [
    "# 4x4 Grid Using SARSA and epsilon-greedy policy (coupled into one environment with Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73a763",
   "metadata": {},
   "source": [
    "Below I propose a class which can do either Q Learning or SARSA on a 4x4 grid example with walls (using the style from https://github.com/viethoangtranduong/reinforcement-learning/blob/main/SARSA%20vs%20QL/CartPole_Agent.ipynb for using the environment as a class with a train() method).\n",
    "\n",
    "It uses a grid laid out as below, where the player starts in a random position and has to make their way to 'F' as the end state. The reward received is -1 for every step, though moving through a wall (shown by the lines) gives -10 reward. The dynamics are not deterministic as each action taken has 50% of changing the state.\n",
    " ``` \n",
    " {F, ,  , },\n",
    " {_,_,_ , },\n",
    " { , ,_|, },\n",
    " { , ,  , }\n",
    " ``` \n",
    "\n",
    "To initialize the environment, create an object using grid_4x4_ex() and then use the train() method with all relevant input variables. The trained policy printed by this method displays in each square the index that should be next moved to if in that state. For the input variables see below, though note that 'fr_gap' defines that every 'fr_gap' iterations, a random value from -10 to -1 is assigned to the reward instead of the correct value (to model an adversarial attack), which is discussed in the next section. To remove this effect, set it to 0. Currently this environment works with either SARSA or Q-Learning.\n",
    "Then to test out the trained policy (will be the policy output of the last time train() was called on that object), use test_opt_policy() which prints out a matrix of the average reward when starting in each state, then also the overall average reward from that matrix as an indicator of policy performance.\n",
    "\n",
    "train() input variables: algtype should be a string that is either 'QL' or 'SARSA'. alpha is the step size parameter float, gamma is the future discount factor float. Epsilon is used for SARSA for the epsilon-greedy policy, where it has a epsilon probability of taking a random action (exploration) and 1-epsilon probability of taking the optimum action (exploitation). The number of training episodes is an int 'n', 'fr_gap' is as above and print_flag is set as True as default but can be set to False to stop printing the trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f11aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rndm\n",
    "\n",
    "#Single argument to initialize for the type of algorithm. Either \"QL\" or \"SARSA\"\n",
    "#If using Q-Learning, choice of epsilon is arbitrary but must be provided as an argument.\n",
    "class grid_4x4_ex():\n",
    "    def __init__(self):\n",
    "        #update: Change so this is in train()\n",
    "        #set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward\n",
    "        # {F, , ,  },\n",
    "        # {_,_,_,  },\n",
    "        # { , ,_|, },\n",
    "        # { , , ,  }\n",
    "        self.R = np.array([[-10, -1, -1,-10], [-10, -1, -1, -1], [-10, -1, -1, -1], [-10,-10, -1, -1],\n",
    "                           [ -1, -1,-10,-10], [ -1, -1,-10, -1], [ -1, -1,-10, -1], [ -1,-10, -1, -1],\n",
    "                           [-10, -1, -1,-10], [-10, -1, -1, -1], [-10,-10,-10, -1], [ -1,-10, -1,-10],\n",
    "                           [ -1, -1,-10,-10], [ -1, -1,-10, -1], [-10, -1,-10, -1], [ -1,-10,-10, -1]])\n",
    "\n",
    "    \n",
    "    #Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "    def choose_A(self, arr, isExploit):\n",
    "        if isExploit:\n",
    "            indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "            return indices[0][0]\n",
    "        else:\n",
    "            return rndm.randint(0,3)\n",
    "\n",
    "    #takes input of action and returns new state according to board physics. Isn't deterministic.\n",
    "    #opt_pol_mode as True forces a deterministic change_state() to help figure out the optimum policy matrix.\n",
    "    def step(self, S, A, opt_pol_mode):\n",
    "        #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "        doMove = rndm.randint(0,1) #50% chance of action actually changing state\n",
    "        \n",
    "        if((S % 4 == 0 and A == 3) or (S < 4 and A == 0) or (S > 11 and A == 2) or (S % 4 == 3 and A == 1) or (doMove == 0 and not opt_pol_mode)):\n",
    "            return S \n",
    "        elif(A == 0):\n",
    "            return S-4\n",
    "        elif(A == 1):\n",
    "            return S+1\n",
    "        elif(A == 2):\n",
    "            return S+4\n",
    "        elif(A == 3):\n",
    "            return S-1\n",
    "        else:\n",
    "            print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "            return S\n",
    "\n",
    "    #trains a policy based on the algtype algorithm given and other parameters\n",
    "    def train(self, algtype, alpha, gamma, epsilon, n, fr_gap, print_flag=True):\n",
    "        if algtype not in {\"QL\", \"SARSA\"}:\n",
    "            print(\"Invalid algorithm type, needs to be 'QL' or 'SARSA'. Default is 'QL'\")\n",
    "            self.algtype = \"QL\"\n",
    "        else:\n",
    "            self.algtype = type\n",
    "        \n",
    "        Q = np.zeros((16,4))  #arbitrary initial Q Table estimate ((16 states, 4 actions) value function)\n",
    "        S0_arr = np.random.randint(0,16,n)  #Generate starting states (0 to 15)\n",
    "        step_mode = False #Uses the step() function for it's purpose within the algorithm as non-deterministic.\n",
    "        for i in range(n):\n",
    "            S = S0_arr[i]          #choose random starting state\n",
    "            Snew = S               #Set Snew to fix the algorithm\n",
    "            running = True         #true until the terminal state is reached (S = 0)\n",
    "            iteration = 1\n",
    "\n",
    "            #SARSA version uses epsilon greedy policy to update Q, Q-Learning uses the greedy policy\n",
    "            #Epsilon=0 is pure exploitation, 1 is pure exploration.\n",
    "            #Both algorithms use an epsilon greedy policy to select the current action\n",
    "            isExploit = True\n",
    "            while running:\n",
    "                if S == 0:\n",
    "                    running = False\n",
    "                else:\n",
    "                    rand_num = rndm.uniform(0,1)\n",
    "                    isExploit = rand_num > epsilon           #epsilon-greedy\n",
    "                    Amax = self.choose_A(Q[S,:], isExploit)  #Get action for current move\n",
    "                    Snew = self.step(S, Amax, step_mode)     #Find out the new state according to the action\n",
    "\n",
    "                    isExploit = not (self.algtype == \"SARSA\" and not isExploit)\n",
    "                    Anewmax = self.choose_A(Q[Snew,:], isExploit) #Get action to update Q with\n",
    "\n",
    "                    if(fr_gap != 0 and iteration % fr_gap == 0):\n",
    "                        Q[S,Amax] = Q[S,Amax] + alpha*(rndm.randint(-10,-1) + gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                    else:\n",
    "                        Q[S,Amax] = Q[S,Amax] + alpha*(self.R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                    S = Snew                         #Update state\n",
    "                    iteration += 1\n",
    "\n",
    "        #Finding the optimum policy from Q table:\n",
    "        self.optimum_pi = np.zeros((16,1))\n",
    "        for i in range(16):\n",
    "            opt_pol_mode = True #forces a deterministic step() to figure out the optimum policy matrix\n",
    "            self.optimum_pi[i] = self.step(i, self.choose_A(Q[i,:],isExploit=True), opt_pol_mode)\n",
    "        if(print_flag):\n",
    "            print(\"optimum policy:\\n {}\".format(self.optimum_pi.reshape(4,4)))\n",
    "        return self.optimum_pi\n",
    "\n",
    "    #For a given Snew and S, find the action that moves between the two.\n",
    "    def find_A(self, S, Snew):\n",
    "        diff = (Snew-S).astype(int)\n",
    "        diff_mapping = {\n",
    "        -4: 0,\n",
    "         1: 1,\n",
    "         4: 2,\n",
    "        -1: 3\n",
    "        }\n",
    "        return diff_mapping.get(diff[0], 0) #return action 0 if not found as default\n",
    "\n",
    "\n",
    "    #Does a n_test episodes and counts average reward for each starting square (with a ceiling cut-off for num of tries)\n",
    "    #Then averages that reward over all squares for a single result.\n",
    "    def test_opt_policy(self,print_flag=True):\n",
    "        n_test = 1600              #About 100 tries for each state\n",
    "        S0_arr = np.zeros(n_test)  #Generate starting states by going through all states repeatedly\n",
    "        for i in range(n_test):\n",
    "            S0_arr[i] = i % 16\n",
    "        S0_arr = S0_arr.astype(int)\n",
    "\n",
    "        #results is the total reward from episodes starting in that state, no_of_starts is the total times started in that state.\n",
    "        results = np.zeros((16,1))       \n",
    "        no_of_starts = np.zeros((16,1))\n",
    "\n",
    "        for i in range(n_test):\n",
    "            current_R = 0          #reward for this episode\n",
    "            S = S0_arr[i]          #choose random starting state\n",
    "            no_of_starts[S0_arr[i]] += 1\n",
    "            Snew = S               #Set Snew to fix the algorithm\n",
    "            running = True         #true until the terminal state is reached (S = 0)\n",
    "            iteration = 1\n",
    "            while running:\n",
    "                if S == 0 or iteration == 16: #limit on 16 iterations for an episode before it ends\n",
    "                    running = False\n",
    "                else:\n",
    "                    A = self.find_A(S, self.optimum_pi[S].astype(int)) #use the Snew from the optimum policy matrix then find corresponding action from S to Snew\n",
    "                    current_R += self.R[S,A]         #update reward for this step\n",
    "                    Snew = self.step(S,A,False) #False is so it's not in optimum policy mode\n",
    "                    S = Snew \n",
    "                    iteration += 1\n",
    "            results[S0_arr[i]] += current_R #add the total reward for this episode to the correct starting state location\n",
    "        results = np.divide(results,no_of_starts)\n",
    "        if print_flag:\n",
    "            print(\"Average reward matrix:\\n \",results.reshape(4,4))\n",
    "            print(\"Mean of matrix rewards: \",np.average(results))\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def test_naive_attack(self,print_flag=True):\n",
    "#         n_test = 1600              #About 100 tries for each state\n",
    "#         S0_arr = np.zeros(n_test)  #Generate starting states by going through all states repeatedly\n",
    "#         for i in range(n_test):\n",
    "#             S0_arr[i] = i % 16\n",
    "#         S0_arr = S0_arr.astype(int)\n",
    "\n",
    "#         #results is the total reward from episodes starting in that state, no_of_starts is the total times started in that state.\n",
    "#         results = np.zeros((16,1))       \n",
    "#         no_of_starts = np.zeros((16,1))\n",
    "\n",
    "#         for i in range(n_test):\n",
    "#             current_R = 0          #reward for this episode\n",
    "#             S = S0_arr[i]          #choose random starting state\n",
    "#             no_of_starts[S0_arr[i]] += 1\n",
    "#             Snew = S               #Set Snew to fix the algorithm\n",
    "#             running = True         #true until the terminal state is reached (S = 0)\n",
    "#             iteration = 1\n",
    "#             while running:\n",
    "#                 if S == 0 or iteration == 16: #limit on 16 iterations for an episode before it ends\n",
    "#                     running = False\n",
    "#                 else:\n",
    "#                     A = self.find_A(S, self.optimum_pi[S].astype(int)) #use the Snew from the optimum policy matrix then find corresponding action from S to Snew\n",
    "#                     current_R += self.R[S,A]         #update reward for this step\n",
    "#                     Snew = self.step(S,A,False) #False is so it's not in optimum policy mode\n",
    "#                     S = Snew \n",
    "#                     iteration += 1\n",
    "#             results[S0_arr[i]] += current_R #add the total reward for this episode to the correct starting state location\n",
    "#         results = np.divide(results,no_of_starts)\n",
    "#         if print_flag:\n",
    "#             print(\"Average reward matrix:\\n \",results.reshape(4,4))\n",
    "#             print(\"Mean of matrix rewards: \",np.average(results))\n",
    "#         return results\n",
    "    \n",
    "        \n",
    "    #repeatedly trains (n_repeats times) using the algtype algorithm using the same parameters as train(), then tests them, then accumulates the average reward outputs and finds an overall average\n",
    "    def repeat_train_test(self, algtype, alpha, gamma, epsilon, n_train, fr_grap, n_repeats, print_flag=True):\n",
    "        results = np.zeros((16,1))\n",
    "        for i in range(n_repeats):\n",
    "            print(\"Completing run through\", i)\n",
    "            self.train(algtype, alpha, gamma, epsilon, n_train, fr_grap, print_flag)\n",
    "            results += self.test_opt_policy(print_flag)\n",
    "        results = results/n_repeats\n",
    "        print(\"Average reward matrix:\\n \",results.reshape(4,4))\n",
    "        print(\"Mean of matrix rewards: \",np.average(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c3f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing run through 0\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.87  -3.91  -5.98]\n",
      " [ -1.93  -3.85  -5.93  -7.58]\n",
      " [-14.77 -14.53 -14.84  -9.93]\n",
      " [-14.59 -13.86 -12.97 -11.72]]\n",
      "Mean of matrix rewards:  -8.64125\n",
      "Completing run through 1\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  7.]\n",
      " [ 0.  4.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.83  -3.97  -9.96]\n",
      " [ -1.93  -4.13  -6.29  -8.04]\n",
      " [-14.82 -14.7  -14.75  -9.64]\n",
      " [-14.58 -14.03 -12.63 -11.46]]\n",
      "Mean of matrix rewards:  -8.9225\n",
      "Completing run through 2\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  2.  3.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.9   -4.08  -6.35]\n",
      " [ -2.11  -3.96  -6.    -7.2 ]\n",
      " [-14.88 -14.41 -14.84  -9.92]\n",
      " [-14.74 -14.11 -13.2  -11.75]]\n",
      "Mean of matrix rewards:  -8.715625\n",
      "Completing run through 3\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.97  -3.86  -6.05]\n",
      " [ -2.03  -4.06  -6.05  -7.22]\n",
      " [-14.8  -14.51 -14.78  -9.75]\n",
      " [-14.42 -13.6  -13.04 -11.9 ]]\n",
      "Mean of matrix rewards:  -8.6275\n",
      "Completing run through 4\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.95  -4.16  -5.85]\n",
      " [ -2.08  -3.96  -5.55  -7.58]\n",
      " [-14.88 -14.46 -14.92 -10.13]\n",
      " [-14.63 -13.94 -12.96 -11.65]]\n",
      "Mean of matrix rewards:  -8.66875\n",
      "Completing run through 5\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.13  -4.27  -6.14]\n",
      " [ -1.97  -4.31  -5.73  -8.02]\n",
      " [-14.74 -14.55 -14.91 -10.54]\n",
      " [-14.58 -14.   -13.11 -11.62]]\n",
      "Mean of matrix rewards:  -8.78875\n",
      "Completing run through 6\n",
      "optimum policy:\n",
      " [[ 0.  0.  6.  7.]\n",
      " [ 5.  1.  5.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.06  -7.35 -10.09]\n",
      " [ -6.09  -3.87  -5.78  -8.43]\n",
      " [-14.89 -14.55 -14.77 -10.04]\n",
      " [-14.61 -13.95 -12.81 -11.46]]\n",
      "Mean of matrix rewards:  -9.421875\n",
      "Completing run through 7\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.1   -4.21  -6.45]\n",
      " [ -2.08  -4.02  -6.29  -8.1 ]\n",
      " [-14.87 -14.32 -14.72  -9.89]\n",
      " [-14.27 -14.35 -13.08 -11.14]]\n",
      "Mean of matrix rewards:  -8.743125\n",
      "Completing run through 8\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  5.  3.]\n",
      " [12.  8.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.05  -3.91  -6.07]\n",
      " [ -1.99  -4.13  -5.82  -8.43]\n",
      " [-14.75 -14.99 -15.    -9.8 ]\n",
      " [-14.58 -13.82 -12.94 -11.98]]\n",
      "Mean of matrix rewards:  -8.76625\n",
      "Completing run through 9\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  7.]\n",
      " [ 0.  4.  2.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.95  -3.96  -9.57]\n",
      " [ -1.81  -4.05  -5.72  -8.54]\n",
      " [-14.78 -14.46 -14.74  -9.34]\n",
      " [-14.63 -13.95 -12.83 -11.49]]\n",
      "Mean of matrix rewards:  -8.86375\n",
      "Average reward matrix:\n",
      "  [[  0.     -1.981  -4.368  -7.251]\n",
      " [ -2.402  -4.034  -5.916  -7.914]\n",
      " [-14.818 -14.548 -14.827  -9.898]\n",
      " [-14.563 -13.961 -12.957 -11.617]]\n",
      "Mean of matrix rewards:  -8.8159375\n"
     ]
    }
   ],
   "source": [
    "algtype = \"QL\" #Either QL or SARSA works\n",
    "alpha = 0.5       #step parameter\n",
    "gamma = 0.9       #discount factor\n",
    "epsilon = 0.01    #exploration parameter (0 for pure exploitation, 1 for pure exploration)\n",
    "n = 16000          #number of episodes \n",
    "fr_gap = 0        #gap between iterations for false rewards provided (0 for no false rewards)\n",
    "n_repeats = 10\n",
    "print_flag = True\n",
    "\n",
    "test_env = grid_4x4_ex()\n",
    "# test_env.train(algtype,alpha,gamma,epsilon,n,fr_gap)\n",
    "# test_env.test_opt_policy()\n",
    "test_env.repeat_train_test(algtype, alpha, gamma, epsilon, n, fr_gap, n_repeats, print_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443e8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing run through 0\n",
      "optimum policy:\n",
      " [[ 0.  0.  6.  7.]\n",
      " [ 0.  4.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.71  -7.74  -9.81]\n",
      " [ -1.91  -3.79  -6.16  -8.34]\n",
      " [-14.92 -14.62 -14.84 -10.03]\n",
      " [-14.46 -14.01 -12.67 -11.4 ]]\n",
      "Mean of matrix rewards:  -9.150625000000002\n",
      "Completing run through 1\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [ 8. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.04  -4.06  -5.59]\n",
      " [ -2.01  -3.76  -5.89  -7.92]\n",
      " [-14.83 -14.46 -14.83  -9.64]\n",
      " [-14.93 -13.84 -12.84 -11.3 ]]\n",
      "Mean of matrix rewards:  -8.62125\n",
      "Completing run through 2\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.99  -3.6   -5.81]\n",
      " [ -1.84  -4.02  -6.32  -8.47]\n",
      " [-14.74 -14.63 -14.8  -10.08]\n",
      " [-14.31 -13.99 -13.05 -11.4 ]]\n",
      "Mean of matrix rewards:  -8.690625\n",
      "Completing run through 3\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.87  -3.94  -5.97]\n",
      " [ -1.95  -3.74  -5.77  -7.88]\n",
      " [-14.85 -14.33 -14.79  -9.76]\n",
      " [-14.69 -14.06 -13.17 -11.96]]\n",
      "Mean of matrix rewards:  -8.670625\n",
      "Completing run through 4\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.37  -4.17  -5.97]\n",
      " [ -2.14  -4.16  -6.25  -7.67]\n",
      " [-14.86 -14.65 -14.92 -10.43]\n",
      " [-14.72 -13.64 -12.82 -11.42]]\n",
      "Mean of matrix rewards:  -8.761875\n",
      "Completing run through 5\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  3.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.92  -4.24  -5.94]\n",
      " [ -1.88  -4.09  -6.06  -8.06]\n",
      " [-14.81 -14.59 -14.77  -9.86]\n",
      " [-14.58 -14.02 -12.92 -11.83]]\n",
      "Mean of matrix rewards:  -8.723125\n",
      "Completing run through 6\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.13  -3.86  -5.86]\n",
      " [ -1.93  -4.13  -5.85  -7.59]\n",
      " [-14.9  -14.38 -14.82  -9.71]\n",
      " [-14.59 -13.9  -12.87 -11.56]]\n",
      "Mean of matrix rewards:  -8.629999999999999\n",
      "Completing run through 7\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  7.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.82  -3.94 -10.05]\n",
      " [ -2.22  -4.34  -6.    -7.95]\n",
      " [-14.86 -14.58 -14.8   -9.9 ]\n",
      " [-14.48 -13.87 -12.37 -11.25]]\n",
      "Mean of matrix rewards:  -8.901875\n",
      "Completing run through 8\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  2.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -2.09  -3.93  -6.  ]\n",
      " [ -1.99  -4.37  -5.84  -8.25]\n",
      " [-14.89 -14.56 -14.73  -9.48]\n",
      " [-14.58 -13.81 -12.8  -11.7 ]]\n",
      "Mean of matrix rewards:  -8.68875\n",
      "Completing run through 9\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  5.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward matrix:\n",
      "  [[  0.    -1.76  -3.78  -5.52]\n",
      " [ -1.87  -4.13  -6.51  -7.82]\n",
      " [-14.9  -14.55 -14.88 -10.22]\n",
      " [-14.62 -13.86 -13.27 -11.48]]\n",
      "Mean of matrix rewards:  -8.698125000000001\n",
      "Average reward matrix:\n",
      "  [[  0.     -1.97   -4.326  -6.652]\n",
      " [ -1.974  -4.053  -6.065  -7.995]\n",
      " [-14.856 -14.535 -14.818  -9.911]\n",
      " [-14.596 -13.9   -12.878 -11.53 ]]\n",
      "Mean of matrix rewards:  -8.7536875\n"
     ]
    }
   ],
   "source": [
    "algtype = \"SARSA\" #Either QL or SARSA works\n",
    "test_env.repeat_train_test(algtype, alpha, gamma, epsilon, n, fr_gap, n_repeats, print_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6644bd",
   "metadata": {},
   "source": [
    "# Trying to break the 4x4 Grid by Feeding False Reward Data During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77bec0e",
   "metadata": {},
   "source": [
    "Haven't got a real justification for this kind of attack just yet. Is more just for experimentation.\n",
    "At every fr_gap steps, it changes the reward recieved to be a random number between -1 and -10 inclusive. I use the same number of iterations as that was sufficient to achieve a sensible optimum policy for the majority of attempts, then alpha and gamma are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e959f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  2.  3.]\n",
      " [12.  8.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward test:\n",
      "  [[  0.    -2.11  -3.87  -6.46]\n",
      " [ -2.02  -3.84  -5.77  -7.41]\n",
      " [-14.8  -14.98 -14.98 -10.02]\n",
      " [-14.55 -13.91 -13.08 -11.22]]\n",
      "Overall average reward:  -8.68875\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 3 \n",
    "\n",
    "test_env = grid_4x4_ex()\n",
    "test_env.train(algtype,alpha,gamma,epsilon,n,fr_gap)\n",
    "test_env.test_opt_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cb100",
   "metadata": {},
   "source": [
    "Can see that it definitely confuses a few of the actions and is no longer often the optimum policy. The walls are sometimes broken and the shortest route isn't always taken. If we change to n=1000 we get better routes (shortest routes) but still the walls are broken. Trying it with a larger gap produces less of a noticeable effect as expected, though also many iterations will terminate before having a false reward value inserted in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25656b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy:\n",
      " [[ 0.  0.  1.  7.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [ 9. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "Average reward test:\n",
      "  [[  0.    -2.02  -4.12 -10.24]\n",
      " [ -2.18  -3.8   -5.87  -7.82]\n",
      " [-14.83 -14.48 -14.83  -9.53]\n",
      " [-14.58 -13.76 -12.98 -11.62]]\n",
      "Overall average reward:  -8.91625\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 5\n",
    "test_env.train(algtype,alpha,gamma,epsilon,n,fr_gap)\n",
    "test_env.test_opt_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c19989",
   "metadata": {},
   "source": [
    "Having a fr_gap of only 2 or 1 (1 renders the algorithm fairly useless for avoiding walls as fr_gap=1 would be changing every reward) the results of the algorithm as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f20fb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_gap is 2\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "fr_gap is 1\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [ 4.  5.  6. 10.]\n",
      " [ 8. 14. 10. 14.]]\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 2\n",
    "print(\"fr_gap is {}\".format(fr_gap))\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)\n",
    "fr_gap = 1\n",
    "print(\"fr_gap is {}\".format(fr_gap))\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf95d99",
   "metadata": {},
   "source": [
    "# Try to break the 4x4 Cell example using false state data during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342f9ed",
   "metadata": {},
   "source": [
    "# Modelling Other Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24fe96",
   "metadata": {},
   "source": [
    "As it turns out, it is more likely to have attacks occuring at testing time (deployment is slightly different but similar, so considered the same here). I've changed the way the class works so that the grid environment is a seperate object that provides the state and reward data via an observation like in openAI gym. Then a man-in-the-middle attack can occur by providing fake data. The code is shown below for the new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deec6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.02887721  -4.72059836  -6.91848014]\n",
      " [ -2.39427117  -4.0363817   -5.94403775  -8.42038714]\n",
      " [-17.80230773 -16.99393061 -17.9465619  -10.39351889]\n",
      " [-17.05881219 -15.74055853 -14.24094067 -12.20629471]]\n",
      "Mean of matrix rewards:  -9.80287241915505\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.46118873  -4.45219703  -6.60727732]\n",
      " [ -2.10707022  -4.01121781  -5.95237856  -7.98373187]\n",
      " [-17.88081735 -17.12902369 -17.99932556  -9.99958801]\n",
      " [-17.13650894 -15.53403059 -13.67193627 -11.79599337]]\n",
      "Mean of matrix rewards:  -9.670142831275385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rndm\n",
    "\n",
    "#Single argu  ment to initialize for the type of algorithm. Either \"QL\" or \"SARSA\"\n",
    "#If using Q-Learning, choice of epsilon is arbitrary but must be provided as an argument.\n",
    "class grid_4x4_ex():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    #Used for changing the environment after initializing\n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    #Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "    def choose_A(self, arr, isExploit):\n",
    "        if isExploit:\n",
    "            indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "            return indices[0][0]\n",
    "        else:\n",
    "            return rndm.randint(0,3)\n",
    "\n",
    "    #trains a policy based on the algtype algorithm given and other parameters\n",
    "    def train(self, algtype, alpha, gamma, epsilon, n, fr_gap, print_flag=True):\n",
    "        if algtype not in {\"QL\", \"SARSA\"}:\n",
    "            print(\"Invalid algorithm type, needs to be 'QL' or 'SARSA'. Default is 'QL'\")\n",
    "            self.algtype = \"QL\"\n",
    "        else:\n",
    "            self.algtype = type\n",
    "        Q = np.zeros((16,4))  #arbitrary initial Q Table estimate ((16 states, 4 actions) value function)\n",
    "        \n",
    "        for i in range(n):\n",
    "            S = self.env.reset()          #choose random starting state\n",
    "            Snew = S                      #initialise Snew\n",
    "            done = False         #false until the terminal state is reached (S = 0)\n",
    "            iteration = 1\n",
    "\n",
    "            #SARSA version uses epsilon greedy policy to update Q, Q-Learning uses the greedy policy\n",
    "            #Epsilon=0 is pure exploitation, 1 is pure exploration.\n",
    "            #Both algorithms use an epsilon greedy policy to select the current action but differ in how Q is updated\n",
    "            isExploit = True\n",
    "            while not done and iteration < 40:\n",
    "                isExploit = rndm.uniform(0,1) > epsilon  #epsilon-greedy\n",
    "                Amax = self.choose_A(Q[S,:], isExploit)  #Get action for current move\n",
    "                Snew, R, done = self.env.step(Amax)     #Find out the new state according to the action\n",
    "\n",
    "                isExploit = not (self.algtype == \"SARSA\" and not isExploit)\n",
    "                Anewmax = self.choose_A(Q[Snew,:], isExploit) #Get action to update Q with\n",
    "\n",
    "                if(fr_gap != 0 and iteration % fr_gap == 0):\n",
    "                    Q[S,Amax] = Q[S,Amax] + alpha*(rndm.randint(-10,-1) + gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                else:\n",
    "                    Q[S,Amax] = Q[S,Amax] + alpha*(R + gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                S = Snew #Update agent's model of the state\n",
    "                iteration += 1\n",
    "\n",
    "        #Finding the optimum policy from Q table. This is formulated as a matrix, where each state has the optimum action in it\n",
    "        self.optimum_pi = np.zeros((16,1))\n",
    "        for i in range(1,16):\n",
    "            isExploit = True\n",
    "            A = self.choose_A(Q[i,:],isExploit)\n",
    "            self.optimum_pi[i] = A\n",
    "        if(print_flag):\n",
    "            print(\"optimum policy:\\n {}\".format(self.optimum_pi.reshape(4,4)))\n",
    "        return self.optimum_pi\n",
    "\n",
    "    #For a given Snew and S, find the action that moves between the two.\n",
    "    def find_A(self, S, Snew):\n",
    "        diff = (Snew-S).astype(int)\n",
    "        diff_mapping = {\n",
    "        -4: 0,\n",
    "         1: 1,\n",
    "         4: 2,\n",
    "        -1: 3\n",
    "        }\n",
    "        return diff_mapping.get(diff[0], 0) #return action 0 if not found as default\n",
    "\n",
    "\n",
    "    #Does a n_test episodes and counts average reward for each starting square (with a ceiling cut-off for num of tries)\n",
    "    #Then averages that reward over all squares for a single result.\n",
    "    def test_opt_policy(self, n_test, print_flag=True):\n",
    "        #results is the total reward from episodes starting in that state, no_of_starts is the total times started in that state.\n",
    "        results = np.zeros((16,1))       \n",
    "        no_of_starts = np.zeros((16,1))\n",
    "        \n",
    "        for i in range(n_test):\n",
    "            acc_R = 0                   #accumulated reward for this episode\n",
    "            start_S = self.env.reset()  #choose random starting state\n",
    "            Snew = S = start_S               \n",
    "            no_of_starts[S] += 1\n",
    "            done = False     #False until the terminal state is reached (S = 0)\n",
    "            iteration = 1\n",
    "            while not done and iteration < 20:\n",
    "                A = self.optimum_pi[S].astype(int) #use the Snew from the optimum policy matrix then find corresponding action from S to Snew\n",
    "                Snew, R, done = self.env.step(A) \n",
    "                acc_R += R       \n",
    "                S = Snew          \n",
    "                iteration += 1\n",
    "            results[start_S] += acc_R #add the total reward for this episode to the correct starting state location\n",
    "        #divide accumulated R for each state by number of starts in each state\n",
    "        results = np.divide(results, no_of_starts, out=np.zeros_like(results), where=no_of_starts!=0)\n",
    "        if print_flag:\n",
    "            print(\"Average reward matrix:\\n \",results.reshape(4,4))\n",
    "            print(\"Mean of matrix rewards: \",np.average(results))\n",
    "        return results\n",
    "        \n",
    "    #repeatedly trains (n_repeats times) using the algtype algorithm using the same parameters as train(), then tests them, then accumulates the average reward outputs and finds an overall average\n",
    "    def repeat_train_test(self, algtype, alpha, gamma, epsilon, n_train, fr_grap, n_repeats, n_test, print_flag=True):\n",
    "        results = np.zeros((16,1))\n",
    "        for i in range(n_repeats):\n",
    "            print(\"Completing run through\", i)\n",
    "            self.train(algtype, alpha, gamma, epsilon, n_train, fr_grap, print_flag)\n",
    "            results += self.test_opt_policy(n_test, print_flag)\n",
    "        results = results/n_repeats\n",
    "        print(\"Average reward matrix:\\n \",results.reshape(4,4))\n",
    "        print(\"Mean of matrix rewards: \",np.average(results))\n",
    "        return np.average(results)\n",
    "    \n",
    "class grid_env:\n",
    "    def __init__(self):\n",
    "        #set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward\n",
    "        # {F, , ,  },\n",
    "        # {_,_,_,  },\n",
    "        # { , ,_|, },\n",
    "        # { , , ,  }\n",
    "        self.R = np.array([[-10, -1, -1,-10], [-10, -1, -1, -1], [-10, -1, -1, -1], [-10,-10, -1, -1],\n",
    "                        [ -1, -1,-10,-10], [ -1, -1,-10, -1], [ -1, -1,-10, -1], [ -1,-10, -1, -1],\n",
    "                        [-10, -1, -1,-10], [-10, -1, -1, -1], [-10,-10,-10, -1], [ -1,-10, -1,-10],\n",
    "                        [ -1, -1,-10,-10], [ -1, -1,-10, -1], [-10, -1,-10, -1], [ -1,-10,-10, -1]])\n",
    "        self.S = np.random.randint(0,16)\n",
    "    \n",
    "    #takes input of action and returns new state according to board physics, alongside the reward of the current state\n",
    "    #opt_pol_mode as True forces a deterministic change_state() to help figure out the optimum policy matrix.\n",
    "    def step(self, A):\n",
    "        #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "        doMove = rndm.randint(0,1) #50% chance of action actually changing state\n",
    "        Snew = self.S                   #Returned new state\n",
    "        if((Snew % 4 == 0 and A == 3) or (Snew < 4 and A == 0) or (Snew > 11 and A == 2) or (Snew % 4 == 3 and A == 1) or (doMove == 0)):\n",
    "            pass \n",
    "        elif(A == 0):\n",
    "            Snew = Snew-4\n",
    "        elif(A == 1):\n",
    "            Snew = Snew+1\n",
    "        elif(A == 2):\n",
    "            Snew = Snew+4\n",
    "        elif(A == 3):\n",
    "            Snew = Snew-1\n",
    "        else:\n",
    "            print('change_state() broken at state {} and action {}'.format(Snew,A))\n",
    "        R = self.R[self.S,A]\n",
    "        self.S = Snew           #keep track of state\n",
    "        done = (Snew == 0)           #done if S is at state 0\n",
    "        return Snew, R, done  #return state for next step and current reward\n",
    "\n",
    "    #Starts an episode within the environment and returns the state.\n",
    "    def reset(self):\n",
    "        self.S = np.random.randint(1,16)  #Generate random start state (1 to 15)\n",
    "        return self.S\n",
    "\n",
    "algtype = \"SARSA\"    #Either QL or SARSA works\n",
    "alpha = 0.5       #step parameter\n",
    "gamma = 0.9       #discount factor\n",
    "epsilon = 0.05    #exploration parameter (0 for pure exploitation, 1 for pure exploration)\n",
    "n = 1600         #number of episodes \n",
    "fr_gap = 0        #gap between iterations for false rewards provided (0 for no false rewards)\n",
    "n_repeats = 10\n",
    "print_flag = False\n",
    "n_test = 1600\n",
    "\n",
    "test_env = grid_4x4_ex(grid_env())\n",
    "print(\"Using SARSA:\")\n",
    "result = test_env.repeat_train_test(algtype,alpha,gamma,epsilon,n,fr_gap,n_repeats,n_test,print_flag)\n",
    "\n",
    "algtype = \"QL\"\n",
    "print(\"Using QL:\")\n",
    "result = test_env.repeat_train_test(algtype,alpha,gamma,epsilon,n,fr_gap,n_repeats,n_test,print_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f37b2",
   "metadata": {},
   "source": [
    "I'm interested in path-finding attacks. Could extend to control environments but we shall see (Review (Lin Y-C, Hong Z-W, Liao Y-H, Shih M-L, Liu M-Y, Sun M (2017) Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748 Liu J, Niu W, Liu J, Zhao J) for sparse reward matrices).\n",
    "\n",
    "Below I start with completely randomizing the state data to make the target agent think it is in a random different state, so that hopefully it performs the wrong action. Then I experiment with reducing the frequency of this attack every N iterations and then only attacks when a set amount of damage would occur, to minimise the chance of the target agent being aware/reduce the adversary effort required.\n",
    "The paper I found suggested that adversaries could either do a uniform attack like this one, or optimise against the long term rewards of minimising the target agent's reward while also minimising the adversary's cost (considering the worst possible case for the target in the long term rather than at every iteration). They also mention two other commonly used adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cff494",
   "metadata": {},
   "source": [
    "# Random Noise (Naive) Attack with Reward During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "835aa94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a false reward gap of  0\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.13903782  -3.93811063  -5.8363035 ]\n",
      " [ -2.24641791  -4.24877908  -5.94897132  -8.00023731]\n",
      " [-17.90199327 -17.19742567 -17.86446794 -10.14611562]\n",
      " [-17.40254997 -16.10288617 -14.16658979 -12.00455391]]\n",
      "Mean of matrix rewards:  -9.696527494903123\n",
      "For a false reward gap of  1\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.46592444  -4.75617612  -6.90692821]\n",
      " [-20.80453655  -7.62192796  -6.60063661  -7.8508937 ]\n",
      " [-22.97373107 -24.49582916 -31.41739362 -26.90736504]\n",
      " [-41.58800585 -25.80287779 -37.48854071 -33.56391785]]\n",
      "Mean of matrix rewards:  -18.827792792022116\n",
      "For a false reward gap of  2\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.01002068  -5.28579187  -7.40934151]\n",
      " [ -2.49345924  -4.24999997  -6.47092093  -7.96364675]\n",
      " [-23.13038018 -24.29435193 -25.89007844  -9.84376726]\n",
      " [-24.34737637 -24.19574356 -20.97622727 -13.78728218]]\n",
      "Mean of matrix rewards:  -12.646774259144962\n",
      "For a false reward gap of  3\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.39538163  -4.37681888  -6.85950244]\n",
      " [ -1.91251332  -4.00047515  -6.70561541  -8.2218286 ]\n",
      " [-18.52133639 -18.35546512 -20.81511918 -10.38938493]\n",
      " [-18.75410689 -17.94691851 -15.20069384 -12.23531188]]\n",
      "Mean of matrix rewards:  -10.418154512084755\n",
      "For a false reward gap of  4\n",
      "Completing run through 0\n",
      "Completing run through 1\n",
      "Completing run through 2\n",
      "Completing run through 3\n",
      "Completing run through 4\n",
      "Completing run through 5\n",
      "Completing run through 6\n",
      "Completing run through 7\n",
      "Completing run through 8\n",
      "Completing run through 9\n",
      "Average reward matrix:\n",
      "  [[  0.          -3.98899584  -7.57487902  -8.2949786 ]\n",
      " [ -3.17187953  -4.00215053  -6.43500008  -8.0170052 ]\n",
      " [-19.17205887 -18.25017872 -18.95530611 -11.84271737]\n",
      " [-18.4351649  -17.40173615 -15.351795   -13.49280457]]\n",
      "Mean of matrix rewards:  -10.899165655761909\n"
     ]
    }
   ],
   "source": [
    "#Firstly train the model; trying both QL and SARSA\n",
    "algtype = \"SARSA\"    #Either QL or SARSA works\n",
    "alpha = 0.5       #step parameter\n",
    "gamma = 0.9       #discount factor\n",
    "epsilon = 0.05    #exploration parameter (0 for pure exploitation, 1 for pure exploration)\n",
    "n = 1600         #number of episodes \n",
    "fr_gap = 0        #gap between iterations for false rewards provided (0 for no false rewards)\n",
    "n_repeats = 10\n",
    "print_flag = False\n",
    "n_test = 1600\n",
    "\n",
    "result = np.zeros((5,1))\n",
    "for i in range(5):\n",
    "    fr_gap = i\n",
    "    print(\"For a false reward gap of \", fr_gap)\n",
    "    result[i] = test_env.repeat_train_test(algtype,alpha,gamma,epsilon,n,fr_gap,n_repeats,n_test,print_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae87bf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.69652749],\n",
       "       [-18.82779279],\n",
       "       [-12.64677426],\n",
       "       [-10.41815451],\n",
       "       [-10.89916566]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eccd22",
   "metadata": {},
   "source": [
    "# Random Noise (Naive) Attack with State During Testing (Black Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049e568",
   "metadata": {},
   "source": [
    "Class below is the adversarial agent acting in a man in the middle attack, to provide the false state data to the training algorithm. It tells the testing (target) agent that it's in a random different state every att_freq iterations, to get a different action than optimum to be produced. We can visualize this as in the figure from \"Learning adversarial attack policies through multi-objective reinforcement learning\" - except the rand_noise_adv_env class just acts as an interface to the target agent that incorporates the actions of both the environment and the adversarial agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceeab2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_noise_adv_env:\n",
    "    def __init__(self, att_gap):\n",
    "        #set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward\n",
    "        # {F, , ,  },\n",
    "        # {_,_,_,  },\n",
    "        # { , ,_|, },\n",
    "        # { , , ,  }\n",
    "        self.R = np.array([[-10, -1, -1,-10], [-10, -1, -1, -1], [-10, -1, -1, -1], [-10,-10, -1, -1],\n",
    "                            [ -1, -1,-10,-10], [ -1, -1,-10, -1], [ -1, -1,-10, -1], [ -1,-10, -1, -1],\n",
    "                            [-10, -1, -1,-10], [-10, -1, -1, -1], [-10,-10,-10, -1], [ -1,-10, -1,-10],\n",
    "                            [ -1, -1,-10,-10], [ -1, -1,-10, -1], [-10, -1,-10, -1], [ -1,-10,-10, -1]])\n",
    "        self.S = np.random.randint(0,16)\n",
    "        self.att_gap = att_gap\n",
    "    \n",
    "    #takes input of action and returns new state according to board physics, alongside the reward of the current state\n",
    "    #opt_pol_mode as True forces a deterministic change_state() to help figure out the optimum policy matrix.\n",
    "    def step(self, A):\n",
    "        #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "        doMove = rndm.randint(0,1) #50% chance of action actually changing state\n",
    "        Snew = self.S                   #Returned new state\n",
    "        if((Snew % 4 == 0 and A == 3) or (Snew < 4 and A == 0) or (Snew > 11 and A == 2) or (Snew % 4 == 3 and A == 1) or (doMove == 0)):\n",
    "            pass \n",
    "        elif(A == 0):\n",
    "            Snew = Snew-4\n",
    "        elif(A == 1):\n",
    "            Snew = Snew+1\n",
    "        elif(A == 2):\n",
    "            Snew = Snew+4\n",
    "        elif(A == 3):\n",
    "            Snew = Snew-1\n",
    "        else:\n",
    "            print('change_state() broken at state {} and action {}'.format(Snew,A))\n",
    "        R = self.R[self.S,A]\n",
    "        self.S = Snew           #keep track of state\n",
    "        done = (Snew == 0)           #done if S is at state 0\n",
    "        #After keeping track of the actual state, it provides fake state data to persuade the agent algorithm to give a suboptimal action\n",
    "        if(self.counter % self.att_gap == 0):\n",
    "            Snew = rndm.randint(1,15)\n",
    "        self.counter += 1\n",
    "        return Snew, R, done  #return state for next step and current reward\n",
    "\n",
    "    #Starts an episode within the environment and returns the state.\n",
    "    def reset(self):\n",
    "        self.S = rndm.randint(1,15)  #Generate random start state (1 to 15)\n",
    "        self.counter = 1                  #initializes a counter\n",
    "        return self.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0d35ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target agent:\n",
      "optimum policy:\n",
      " [[0. 3. 3. 3.]\n",
      " [0. 3. 3. 3.]\n",
      " [2. 2. 3. 0.]\n",
      " [1. 1. 1. 0.]]\n",
      "\n",
      "Testing the target agent on an ordinary environment:\n",
      "Average reward matrix:\n",
      "  [[  0.          -1.82608696  -4.25688073  -5.71      ]\n",
      " [ -2.5         -3.57142857  -6.07272727  -8.0776699 ]\n",
      " [-17.73913043 -16.69158879 -17.65384615  -9.84158416]\n",
      " [-17.06730769 -15.15740741 -13.88983051 -11.77570093]]\n",
      "Mean of matrix rewards:  -9.489449344524726\n",
      "\n",
      "Testing attack with gap 1\n",
      "Average reward matrix:\n",
      "  [[  0.         -13.56603774 -34.18699187 -49.        ]\n",
      " [-33.         -55.84466019 -54.46956522 -57.88297872]\n",
      " [-96.02678571 -81.22580645 -72.85714286 -67.89215686]\n",
      " [-89.19607843 -83.40650407 -78.64646465 -81.2991453 ]]\n",
      "Mean of matrix rewards:  -59.28126987928424\n",
      "\n",
      "Testing attack with gap 2\n",
      "Average reward matrix:\n",
      "  [[  0.          -4.23423423  -9.65       -17.56880734]\n",
      " [ -7.72881356 -13.34513274 -25.51694915 -24.24444444]\n",
      " [-49.18691589 -41.94545455 -43.04273504 -31.30851064]\n",
      " [-45.44554455 -44.23232323 -43.32142857 -40.83168317]]\n",
      "Mean of matrix rewards:  -27.60018606963859\n",
      "\n",
      "Testing attack with gap 3\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.67708333  -6.3627451  -10.39669421]\n",
      " [ -4.33333333 -10.28865979 -14.17355372 -19.43103448]\n",
      " [-35.88990826 -32.69072165 -34.88495575 -22.58695652]\n",
      " [-34.96363636 -33.63157895 -32.31460674 -29.67010309]]\n",
      "Mean of matrix rewards:  -20.268473206302584\n",
      "\n",
      "Testing attack with gap 4\n",
      "Average reward matrix:\n",
      "  [[  0.          -1.93636364  -5.16161616  -8.97457627]\n",
      " [ -2.27868852  -7.10185185 -10.75454545 -14.55140187]\n",
      " [-29.11650485 -28.95833333 -30.10843373 -19.47169811]\n",
      " [-27.70909091 -27.57142857 -26.08       -21.55963303]]\n",
      "Mean of matrix rewards:  -16.333385394575288\n",
      "\n",
      "Testing attack with gap 5\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.47058824  -4.46666667  -7.93684211]\n",
      " [ -2.94392523  -5.34146341  -8.72897196 -13.40566038]\n",
      " [-25.82954545 -25.19607843 -26.09734513 -14.75862069]\n",
      " [-25.77358491 -24.50961538 -20.72727273 -20.74789916]]\n",
      "Mean of matrix rewards:  -14.308379992562948\n",
      "\n",
      "Testing attack with gap 6\n",
      "Average reward matrix:\n",
      "  [[  0.          -2.32110092  -4.10679612  -6.87962963]\n",
      " [ -2.03809524  -5.875       -7.26530612 -11.65656566]\n",
      " [-26.1980198  -25.17117117 -26.69       -15.27102804]\n",
      " [-25.79508197 -24.58974359 -18.51886792 -17.27272727]]\n",
      "Mean of matrix rewards:  -13.728070840338898\n",
      "\n",
      "Testing attack with gap 7\n",
      "Average reward matrix:\n",
      "  [[  0.          -1.86538462  -4.03759398  -7.39784946]\n",
      " [ -2.37254902  -5.05607477  -7.78        -9.34951456]\n",
      " [-25.2605042  -21.80373832 -23.90909091 -13.48913043]\n",
      " [-23.39130435 -22.13043478 -19.792      -13.11926606]]\n",
      "Mean of matrix rewards:  -12.547152216285891\n"
     ]
    }
   ],
   "source": [
    "#First train the target agent and test it on an ordinary environment:\n",
    "print(\"Training target agent:\")\n",
    "algtype = \"QL\"    #Either QL or SARSA works\n",
    "alpha = 0.5       #step parameter\n",
    "gamma = 0.9       #discount factor\n",
    "epsilon = 0.05    #exploration parameter (0 for pure exploitation, 1 for pure exploration)\n",
    "n = 1600          #number of episodes \n",
    "fr_gap = 0        #gap between iterations for false rewards provided (0 for no false rewards)\n",
    "print_flag = True\n",
    "n_test = 1600\n",
    "target_agent = grid_4x4_ex(grid_env())\n",
    "target_agent.train(algtype, alpha, gamma, epsilon, n, fr_gap, print_flag)\n",
    "print(\"\\nTesting the target agent on an ordinary environment:\")\n",
    "_suppressed_output = target_agent.test_opt_policy(n_test, print_flag)\n",
    "\n",
    "#Then attack during testing of the trained policy with the adversarial agent acting as a man-in-the-middle\n",
    "#Every att_gap iterations an attack occurs\n",
    "att_gap_range = np.arange(1,8)\n",
    "results = np.zeros((7,1))\n",
    "for att_gap in att_gap_range:\n",
    "    print(\"\\nTesting attack with gap\", att_gap)\n",
    "    target_agent.set_env(rand_noise_adv_env(att_gap))\n",
    "    results[int(att_gap)-1] = np.average(target_agent.test_opt_policy(n_test, print_flag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66f0e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average Accumulated Reward')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(att_gap_range,results)\n",
    "plt.title(\"Effect of Random Noise (State Data, Blackbox) Adversarial Attack on 4x4 Grid \")\n",
    "plt.xlabel(\"Iteration Gap Between Attacks\")\n",
    "plt.ylabel(\"Average Accumulated Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164e30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cb2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a66d80eec2cf75596d5ec2c2e4a612679fac0bfe77f0a2ea851290eecace26d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
