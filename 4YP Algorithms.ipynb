{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68670c24",
   "metadata": {},
   "source": [
    "NoteBook to try some example algorithms in python from the reinforcement learning book. Originally the algorithms I implemented on MATLAB have been translated into python code. \n",
    "The basis problem is a grid of squares where the terminal states are square 1,and the final square. The policy implemented contains a grid of numbers, where in each square is the index the player should move to from that square.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4172c",
   "metadata": {},
   "source": [
    "# The Actual Value Function (for the policy pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c5ddb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 16 is different from 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-078ca57fac05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m              [0,0,0,0,0,0,0,0,1]]) #Prob matrix of policy\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mVActual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Solve bellman equation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mVActual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 16 is different from 9)"
     ]
    }
   ],
   "source": [
    "pi = np.array([0,0,1,0,1,8,7,8,8]) #policy array - each number points to the next location\n",
    "\n",
    "P = np.array([[1,0,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,1,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,0,1]]) #Prob matrix of policy\n",
    "\n",
    "VActual = np.matmul(np.linalg.inv(np.eye(9)-gamma*P),R) #Solve bellman equation\n",
    "VActual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680251d",
   "metadata": {},
   "source": [
    "# TD[0] (One Step TD) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34defc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "V = np.zeros((9,1))                #arbitrary initial V estimate  \n",
    "R = np.ones((9,1))                    \n",
    "R[0], R[8] = 0, 0                  #reward array\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "#initially use for loops though I'm aware it's not the fastest way.\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    running = True         #true until the terminal state is reached\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Snew = pi[S]\n",
    "            V[S] = V[S] + alpha*(R[S]+gamma*V[Snew]-V[S])\n",
    "            S = Snew\n",
    "\n",
    "print(\"The difference between the estimate and the actual value function is : \\n\", V-VActual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a0b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db66e6c2",
   "metadata": {},
   "source": [
    "# Simple 3x3 Grid Q Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23545",
   "metadata": {},
   "source": [
    "Slightly different implementation of the 3x3 grid idea, where the Q table has actions 0-3; up, right, down, left in that order ascending. Rows of the Q table are the states and the columns are the possible actions. This method learns the optimum policy as the iterations of episodes update the Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ec78f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q: [[ 0.          0.          0.          0.        ]\n",
      " [-5.         -1.533125   -1.6090625  -1.        ]\n",
      " [-5.         -5.         -1.8998946  -1.89990885]\n",
      " [-1.         -1.2        -1.67515625 -5.        ]\n",
      " [-1.8975544  -1.89560302 -1.89675097 -1.89659981]\n",
      " [-1.67515625 -5.         -1.         -1.65082812]\n",
      " [-1.89923634 -1.89937899 -5.         -5.        ]\n",
      " [-1.58375    -1.         -5.         -1.18875   ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "optimum policy:\n",
      " [[0. 0. 5.]\n",
      " [0. 5. 8.]\n",
      " [3. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "def get_max_A(arr):\n",
    "    indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "    return indices[0][0]\n",
    "\n",
    "#takes input of action and returns new state according to board physics\n",
    "def change_state(S,A):\n",
    "    #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "    if((S % 3 == 0 and A == 3) or (S < 3 and A == 0) or (S > 5 and A == 2) or (S % 3 == 2 and A == 1)):\n",
    "        return S\n",
    "    elif(A == 0):\n",
    "        return S-3\n",
    "    elif(A == 1):\n",
    "        return S+1\n",
    "    elif(A == 2):\n",
    "        return S+3\n",
    "    elif(A == 3):\n",
    "        return S-1\n",
    "    else:\n",
    "        print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "        return S\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "\n",
    "Q = np.zeros((9,4))                #arbitrary initial Q Table estimate ((9 states, 4 actions) value function)\n",
    "\n",
    "#set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward=-1\n",
    "R = np.array([[-10, -1, -1, -10],\n",
    "              [-10, -1, -1,  -1],\n",
    "              [-10,-10, -1,  -1],\n",
    "              [ -1, -1, -1, -10],\n",
    "              [ -1, -1, -1,  -1],\n",
    "              [ -1,-10, -1,  -1],\n",
    "              [ -1, -1,-10, -10],\n",
    "              [ -1, -1,-10,  -1],\n",
    "              [ -1,-10,-10,  -1]])\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    Snew = S               #Set Snew to fix the algorithm\n",
    "    running = True         #true until the terminal state is reached (S = 0 or 8)\n",
    "    # iteration = 1\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Amax = get_max_A(Q[S,:])       #Get action that maximizes current move\n",
    "            Snew = change_state(S,Amax)    #Find out the new state according to the action\n",
    "            Anewmax = get_max_A(Q[Snew,:]) #Get action that maximizes the next move\n",
    "            Q[S,Amax] = Q[S,Amax] + alpha*(R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "            S = Snew                         #Update state\n",
    "            # print('Iteration {} at state {}'.format(iteration,S)) #debugging iteration check\n",
    "            # iteration += 1\n",
    "\n",
    "print('Final Q: {}'.format(Q))\n",
    "#Find the optimum policy from Q:\n",
    "optimum_pi = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimum_pi[j,i] = change_state(i+j*3,get_max_A(Q[i+j*3,:]))\n",
    "optimum_pi[2,2] = 8\n",
    "print(\"optimum policy:\\n {}\".format(optimum_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1974ec",
   "metadata": {},
   "source": [
    "# More Complicated 4x4 Grid with walls, Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc06e6a",
   "metadata": {},
   "source": [
    "Using a 4x4 grid with some walls as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21f6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {F, , , },\n",
    "# {_,_,_, },\n",
    "# { , ,_|, },\n",
    "# { , , , }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9b603",
   "metadata": {},
   "source": [
    "Where F is the final state (we only have one final state in the index=0 square this time).\n",
    "The optimum policy shown has the index in each square of the square that the player should move to. This algorithm converges to an optimum solution, avoiding the walls via a reward incentive (though movement isn't blocked).\n",
    "Fr_gap defines that every 'fr_gap' iterations, a random value from -10 to -1 is assigned to the reward instead of the correct value, which is discussed in the next section. To remove this effect, set it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea4fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grid_4x4_q_learning_ex(alpha,gamma,n,fr_grap):\n",
    "    #Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "    def get_max_A(arr):\n",
    "        indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "        return indices[0][0]\n",
    "\n",
    "    #takes input of action and returns new state according to board physics\n",
    "    def change_state(S,A):\n",
    "        #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "        if((S % 4 == 0 and A == 3) or (S < 4 and A == 0) or (S > 11 and A == 2) or (S % 4 == 3 and A == 1)):\n",
    "            return S\n",
    "        elif(A == 0):\n",
    "            return S-4\n",
    "        elif(A == 1):\n",
    "            return S+1\n",
    "        elif(A == 2):\n",
    "            return S+4\n",
    "        elif(A == 3):\n",
    "            return S-1\n",
    "        else:\n",
    "            print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "            return S\n",
    "        \n",
    "    Q = np.zeros((16,4))                #arbitrary initial Q Table estimate ((9 states, 4 actions) value function)\n",
    "\n",
    "    #set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward\n",
    "    # {F, , , },\n",
    "    # {_,_,_, },\n",
    "    # { , ,_|, },\n",
    "    # { , , , }\n",
    "    R = np.array([[-10, -1, -1,-10], [-10, -1, -1, -1], [-10, -1, -1, -1], [-10,-10, -1, -1],\n",
    "                  [ -1, -1,-10,-10], [ -1, -1,-10, -1], [ -1, -1,-10, -1], [ -1,-10, -1, -1],\n",
    "                  [-10, -1, -1,-10], [-10, -1, -1, -1], [-10,-10,-10, -1], [ -1,-10, -1,-10],\n",
    "                  [ -1, -1,-10,-10], [ -1, -1,-10, -1], [-10, -1,-10, -1], [ -1,-10,-10, -1]])\n",
    "    S0_arr = np.random.randint(0,16,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "    for i in range(n):\n",
    "        S = S0_arr[i]          #choose random starting state\n",
    "        Snew = S               #Set Snew to fix the algorithm\n",
    "        running = True         #true until the terminal state is reached (S = 0 or 8)\n",
    "        # iteration = 1\n",
    "        while running:\n",
    "            if S == 0:\n",
    "                running = False\n",
    "            else:\n",
    "                Amax = get_max_A(Q[S,:])       #Get action that maximizes current move\n",
    "                Snew = change_state(S,Amax)    #Find out the new state according to the action\n",
    "                Anewmax = get_max_A(Q[Snew,:]) #Get action that maximizes the next move\n",
    "                \n",
    "                if(fr_grap != 0 and iteration % fr_gap == 0):\n",
    "                    Q[S,Amax] = Q[S,Amax] + alpha*(rndm.randint(-10,-1) + gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                else:\n",
    "                    Q[S,Amax] = Q[S,Amax] + alpha*(R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "                S = Snew                         #Update state\n",
    "                # print('Iteration {} at state {}'.format(iteration,S)) #debugging iteration check\n",
    "                # iteration += 1\n",
    "\n",
    "#     print('Final Q: {}'.format(Q))\n",
    "    optimum_pi = np.zeros((4,4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            optimum_pi[j,i] = change_state(i+j*4,get_max_A(Q[i+j*4,:]))\n",
    "    print(\"optimum policy:\\n {}\".format(optimum_pi))\n",
    "    return optimum_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d06c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n"
     ]
    }
   ],
   "source": [
    "#Call the function with defined parameters\n",
    "alpha, gamma, n, fr_gap = 0.5, 0.9, 100, 0    #step parameter, discount factor, number of episodes, gap for false rewards\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6644bd",
   "metadata": {},
   "source": [
    "# Trying to break the 4x4 Q Learning Grid by Feeding False Reward Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77bec0e",
   "metadata": {},
   "source": [
    "At every 3 steps, it changes the reward recieved to be a random number between -1 and -10 inclusive. I use the same number of iterations (100) as the previous one as that was sufficient to achieve a sensible optimum policy for all attempts, then alpha and gamma are the same. The functions get_max_A() and change_state() are already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e959f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  5.  7.]\n",
      " [ 4.  5.  9. 10.]\n",
      " [ 8. 12. 10. 14.]]\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 3\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cb100",
   "metadata": {},
   "source": [
    "Can see that it definitely confuses a few of the actions and is no longer often the optimum policy. The walls are sometimes broken and the shortest route isn't always taken. If we change to n=1000 we get better routes (shortest routes) but still the walls are broken. Trying it with a larger gap produces less of a noticeable effect as expected, though also many iterations will terminate before having a false reward value inserted in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25656b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  5.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 5\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c19989",
   "metadata": {},
   "source": [
    "Having a fr_gap of only 2 or 1 (1 renders the algorithm fairly useless for avoiding walls as fr_gap=1 would be changing every reward) the results of the algorithm as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f20fb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_gap is 2\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  1.  5.  3.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n",
      "fr_gap is 1\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  3.]\n",
      " [ 0.  4.  5.  3.]\n",
      " [ 4.  8. 14. 11.]\n",
      " [ 8.  9. 10. 11.]]\n"
     ]
    }
   ],
   "source": [
    "fr_gap = 2\n",
    "print(\"fr_gap is {}\".format(fr_gap))\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)\n",
    "fr_gap = 1\n",
    "print(\"fr_gap is {}\".format(fr_gap))\n",
    "optimum_policy = grid_4x4_q_learning_ex(alpha,gamma,n,fr_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0300d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a66d80eec2cf75596d5ec2c2e4a612679fac0bfe77f0a2ea851290eecace26d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
