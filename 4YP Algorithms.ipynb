{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68670c24",
   "metadata": {},
   "source": [
    "NoteBook to try some example algorithms in python from the reinforcement learning book. Originally the algorithms I implemented on MATLAB have been translated into python code. \n",
    "The basis problem is a grid of squares where the terminal states are square 1,and the final square. The policy implemented contains a grid of numbers, where in each square is the index the player should move to from that square.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4172c",
   "metadata": {},
   "source": [
    "# The Actual Value Function (for the policy pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72c5ddb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [1.9],\n",
       "       [1. ],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([0,0,1,0,1,8,7,8,8]) #policy array - each number points to the next location\n",
    "\n",
    "P = np.array([[1,0,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,1,0],\n",
    "             [0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,0,1]]) #Prob matrix of policy\n",
    "\n",
    "VActual = np.matmul(np.linalg.inv(np.eye(9)-gamma*P),R) #Solve bellman equation\n",
    "VActual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680251d",
   "metadata": {},
   "source": [
    "# TD[0] (One Step TD) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34defc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the estimate and the actual value function is : \n",
      " [[ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [-1.47249879e-07]\n",
      " [-7.62939453e-06]\n",
      " [-3.70615691e-07]\n",
      " [-4.65661287e-10]\n",
      " [-1.48702093e-06]\n",
      " [-9.09494702e-13]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "V = np.zeros((9,1))                #arbitrary initial V estimate  \n",
    "R = np.ones((9,1))                    \n",
    "R[0], R[8] = 0, 0                  #reward array\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "#initially use for loops though I'm aware it's not the fastest way.\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    running = True         #true until the terminal state is reached\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Snew = pi[S]\n",
    "            V[S] = V[S] + alpha*(R[S]+gamma*V[Snew]-V[S])\n",
    "            S = Snew\n",
    "\n",
    "print(\"The difference between the estimate and the actual value function is : \\n\", V-VActual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a0b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db66e6c2",
   "metadata": {},
   "source": [
    "# Simple 3x3 Grid Q Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23545",
   "metadata": {},
   "source": [
    "Slightly different implementation of the 3x3 grid idea, where the Q table has actions 0-3; up, right, down, left in that order ascending. Rows of the Q table are the states and the columns are the possible actions. This method learns the optimum policy as the iterations of episodes update the Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ec78f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q: [[ 0.          0.          0.          0.        ]\n",
      " [-5.         -1.18875    -1.65617188 -1.        ]\n",
      " [-5.         -5.         -1.89998767 -1.89998779]\n",
      " [-1.         -1.18875    -1.408125   -5.        ]\n",
      " [-1.89051866 -1.89435179 -1.89330679 -1.89407904]\n",
      " [-1.533125   -5.         -1.         -1.31375   ]\n",
      " [-1.89950877 -1.89909814 -5.         -5.        ]\n",
      " [-1.533125   -1.         -5.         -1.2905625 ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "optimum policy:\n",
      " [[0. 0. 5.]\n",
      " [0. 1. 8.]\n",
      " [7. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "def get_max_A(arr):\n",
    "    indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "    return indices[0][0]\n",
    "\n",
    "#takes input of action and returns new state according to board physics\n",
    "def change_state(S,A):\n",
    "    #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "    if((S % 3 == 0 and A == 3) or (S < 3 and A == 0) or (S > 5 and A == 2) or (S % 3 == 2 and A == 1)):\n",
    "        return S\n",
    "    elif(A == 0):\n",
    "        return S-3\n",
    "    elif(A == 1):\n",
    "        return S+1\n",
    "    elif(A == 2):\n",
    "        return S+3\n",
    "    elif(A == 3):\n",
    "        return S-1\n",
    "    else:\n",
    "        print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "        return S\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "\n",
    "Q = np.zeros((9,4))                #arbitrary initial Q Table estimate ((9 states, 4 actions) value function)\n",
    "\n",
    "#set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward=-1\n",
    "R = np.array([[-10, -1, -1, -10],\n",
    "              [-10, -1, -1,  -1],\n",
    "              [-10,-10, -1,  -1],\n",
    "              [ -1, -1, -1, -10],\n",
    "              [ -1, -1, -1,  -1],\n",
    "              [ -1,-10, -1,  -1],\n",
    "              [ -1, -1,-10, -10],\n",
    "              [ -1, -1,-10,  -1],\n",
    "              [ -1,-10,-10,  -1]])\n",
    "S0_arr = np.random.randint(0,9,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    Snew = S               #Set Snew to fix the algorithm\n",
    "    running = True         #true until the terminal state is reached (S = 0 or 8)\n",
    "    # iteration = 1\n",
    "    while running:\n",
    "        if S == 0 or S == 8:\n",
    "            running = False\n",
    "        else:\n",
    "            Amax = get_max_A(Q[S,:])       #Get action that maximizes current move\n",
    "            Snew = change_state(S,Amax)    #Find out the new state according to the action\n",
    "            Anewmax = get_max_A(Q[Snew,:]) #Get action that maximizes the next move\n",
    "            Q[S,Amax] = Q[S,Amax] + alpha*(R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "            S = Snew                         #Update state\n",
    "            # print('Iteration {} at state {}'.format(iteration,S)) #debugging iteration check\n",
    "            # iteration += 1\n",
    "\n",
    "print('Final Q: {}'.format(Q))\n",
    "#Find the optimum policy from Q:\n",
    "optimum_pi = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        optimum_pi[j,i] = change_state(i+j*3,get_max_A(Q[i+j*3,:]))\n",
    "optimum_pi[2,2] = 8\n",
    "print(\"optimum policy:\\n {}\".format(optimum_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1974ec",
   "metadata": {},
   "source": [
    "# More Complicated 4x4 Grid with walls, Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc06e6a",
   "metadata": {},
   "source": [
    "Using a 4x4 grid with some walls as shown below:\n",
    "{F, , , },\n",
    "{_,_,_, },\n",
    "{ , ,_|, },\n",
    "{ , , , }\n",
    "Where F is the final state.\n",
    "The optimum policy shown has the index in each square of the square that the player should move to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea4fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q: [[ 0.          0.          0.          0.        ]\n",
      " [-5.         -1.18875    -1.701875   -1.        ]\n",
      " [-5.         -2.0045625  -2.43918969 -1.9       ]\n",
      " [-5.         -5.         -2.8267977  -2.71      ]\n",
      " [-1.         -1.245      -5.         -5.        ]\n",
      " [-1.9        -1.92909687 -5.         -1.9       ]\n",
      " [-2.71       -2.87998358 -5.         -2.71      ]\n",
      " [-3.439      -5.         -3.85334621 -3.439     ]\n",
      " [-7.94824219 -6.50598787 -6.50125559 -9.75      ]\n",
      " [-8.35158356 -6.19179893 -6.12579511 -6.24083094]\n",
      " [-8.6993882  -9.44557883 -9.75       -6.51313462]\n",
      " [-4.0951     -5.         -4.28579446 -5.        ]\n",
      " [-6.43220859 -6.12564453 -9.75       -9.75      ]\n",
      " [-5.89324849 -5.6953279  -9.75       -5.95273391]\n",
      " [-9.75       -5.217031   -9.75       -5.56159263]\n",
      " [-4.68559    -5.         -5.         -5.01094988]]\n",
      "optimum policy:\n",
      " [[ 0.  0.  1.  2.]\n",
      " [ 0.  4.  2.  6.]\n",
      " [12. 13.  9.  7.]\n",
      " [13. 14. 15. 11.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Takes the arr from Q with one state and multiple actions, finds highest value and returns the index of the action column\n",
    "def get_max_A(arr):\n",
    "    indices = np.where(arr == np.amax(arr)) #indices of maximum Q value of the array\n",
    "    return indices[0][0]\n",
    "\n",
    "#takes input of action and returns new state according to board physics\n",
    "def change_state(S,A):\n",
    "    #Don't move if at the board edge. Otherwise 0 up, 1 right, 2 down, 3 left\n",
    "    if((S % 4 == 0 and A == 3) or (S < 4 and A == 0) or (S > 11 and A == 2) or (S % 4 == 3 and A == 1)):\n",
    "        return S\n",
    "    elif(A == 0):\n",
    "        return S-4\n",
    "    elif(A == 1):\n",
    "        return S+1\n",
    "    elif(A == 2):\n",
    "        return S+4\n",
    "    elif(A == 3):\n",
    "        return S-1\n",
    "    else:\n",
    "        print('change_state() broken at state {} and action {}'.format(S,A))\n",
    "        return S\n",
    "\n",
    "alpha, gamma, n = 0.5, 0.9, 200    #step parameter, discount factor, number of episodes\n",
    "\n",
    "Q = np.zeros((16,4))                #arbitrary initial Q Table estimate ((9 states, 4 actions) value function)\n",
    "\n",
    "#set up reward so if an action would take it off the board, make it stay still and lose 10 reward, otherwise reward\n",
    "# {F, , , },\n",
    "# {_,_,_, },\n",
    "# { , ,_|, },\n",
    "# { , , , }\n",
    "R = np.array([[-10, -1, -1,-10], [-10, -1, -1, -1], [-10, -1, -1, -1], [-10,-10, -1, -1],\n",
    "              [ -1, -1,-10,-10], [ -1, -1,-10, -1], [ -1, -1,-10, -1], [ -1,-10, -1, -1],\n",
    "              [-10, -1, -1,-10], [-10, -1, -1, -1], [-10,-10,-10, -1], [ -1,-10, -1,-10],\n",
    "              [ -1, -1,-10,-10], [ -1, -1,-10, -1], [-10, -1,-10, -1], [ -1,-10,-10, -1]])\n",
    "S0_arr = np.random.randint(0,16,n)  #Generate starting states (1,to 9)\n",
    "\n",
    "for i in range(n):\n",
    "    S = S0_arr[i]          #choose random starting state\n",
    "    Snew = S               #Set Snew to fix the algorithm\n",
    "    running = True         #true until the terminal state is reached (S = 0 or 8)\n",
    "    # iteration = 1\n",
    "    while running:\n",
    "        if S == 0:\n",
    "            running = False\n",
    "        else:\n",
    "            Amax = get_max_A(Q[S,:])       #Get action that maximizes current move\n",
    "            Snew = change_state(S,Amax)    #Find out the new state according to the action\n",
    "            Anewmax = get_max_A(Q[Snew,:]) #Get action that maximizes the next move\n",
    "            Q[S,Amax] = Q[S,Amax] + alpha*(R[S,Amax]+ gamma*Q[Snew,Anewmax] - Q[S,Amax]) #Update Q\n",
    "            S = Snew                         #Update state\n",
    "            # print('Iteration {} at state {}'.format(iteration,S)) #debugging iteration check\n",
    "            # iteration += 1\n",
    "\n",
    "print('Final Q: {}'.format(Q))\n",
    "optimum_pi = np.zeros((4,4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        optimum_pi[j,i] = change_state(i+j*4,get_max_A(Q[i+j*4,:]))\n",
    "print(\"optimum policy:\\n {}\".format(optimum_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6644bd",
   "metadata": {},
   "source": [
    "# Trying to break the 4x4 Q Learning Grid by Feeding False Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77bec0e",
   "metadata": {},
   "source": [
    "In progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959f01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a66d80eec2cf75596d5ec2c2e4a612679fac0bfe77f0a2ea851290eecace26d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
